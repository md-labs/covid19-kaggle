"""
generates the token embeddings generated by a fine-tuned BertForSequenceClassification model
1. loads a fine-tuned model (along with the config) from fine_tuned_result/ folder
2. reads the json files from the extracted/ folder
3. save the input_ids, attention_masks, labels and paper_ids that is to be fed to the model to fine_tuned_result/ folder
4. save the token embeddings in word_embeddings/fine_tuned_result/ folder
5. save the seed embeddings in word_embeddings/fine_tuned_result/ folder

"""
import argparse
import pickle
from collections import defaultdict
import time
import logging
import datetime

from covid19_processor import Covid19Processor, PaperAbstractDataset

import torch
from torch.utils.data import DataLoader, SequentialSampler
from transformers import BertForSequenceClassification, BertConfig, BertTokenizer
import pandas as pd

logger = logging.getLogger(__name__)
logging.basicConfig(
  format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
  datefmt="%m/%d/%Y %H:%M:%S",
  level=logging.INFO)

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
tokens_with_embeddings = set()
seed_words = set()


def format_time(elapsed):
  '''
  Takes a time in seconds and returns a string hh:mm:ss
  '''
  # Round to the nearest second.
  elapsed_rounded = int(round((elapsed)))
  # Format as hh:mm:ss
  return str(datetime.timedelta(seconds=elapsed_rounded))


def add_seed_word(words):
  seed_words.update(words)


def main():
  parser = argparse.ArgumentParser()

  parser.add_argument(
    "--model_and_config_dir",
    type=str,
    required=True,
    help="Path to fine-trained BertForSequenceClassification model and config."
         "Its assumed that the directory contains config.json and pytorch_model.bin files.")

  parser.add_argument(
    "--data_dir",
    default=None,
    type=str,
    help="The input data dir. Should contain the .json files for the task.")

  parser.add_argument(
    "--output_dir",
    default=None,
    type=str,
    required=True,
    help="The output directory where the token embeddings will be written.")

  parser.add_argument(
    "--filter_dir",
    type=str,
    required=False,
    help="The directory which contains the filter files.")

  parser.add_argument(
    "--max_seq_length",
    default=512,
    type=int,
    help="The maximum total input sequence length after WordPiece tokenization. Sequences "
         "longer than this will be truncated, and sequences shorter than this will be padded.")

  parser.add_argument(
    "--batch_size",
    default=4,
    type=int,
    help="The batch size to feed the model")

  parser.add_argument("--have_input_data", action="store_true", help="Whether the input data is already stored in the form of Tensors")

  parser.add_argument('--seed_words', default=None, nargs='+')

  args = parser.parse_args()

  add_seed_word(args.seed_words)
  logger.info("seed words given by user are %s", str(seed_words))

  config = BertConfig.from_pretrained(args.model_and_config_dir)
  model = BertForSequenceClassification.from_pretrained(args.model_and_config_dir, config=config)
  tokenizer = BertTokenizer.from_pretrained('bert-base-cased')

  if not args.have_input_data:
    processor = Covid19Processor()
    json_files = processor.extract_json_file_names(args.data_dir, args.filter_dir)
    logger.info("##### json file names extracted #####")
    X_train, X_test, y_train, y_test = processor.preprocess_data_to_df(json_files)
    logger.info("##### data converted to datafame #####")
    # no training so concatenate the train and test files
    X = pd.concat([X_train, X_test], axis=0)
    y = pd.concat([y_train, y_test], axis=0)
    input_ids, attention_masks, paper_ids_with_abstracts, labels = processor.create_input_ids__attention_masks_tensor(
      args, X, y, tokenizer, args.max_seq_length)
    logger.info("##### data converted to tensors #####")
  else:
    logger.info(f"##### loading input tensors from inputs/{args.output_dir} #####")
    input_ids = torch.load(f"inputs/{args.output_dir}/input_ids.pt")
    attention_masks = torch.load(f"inputs/{args.output_dir}/attention_masks.pt")
    with open(f"inputs/{args.output_dir}/dummy_labels.pickle", "rb") as f:
      labels = pickle.load(f)
    with open(f"inputs/{args.output_dir}/paper_ids.pickle", "rb") as f:
      paper_ids_with_abstracts = pickle.load(f)

  ##### From here this code models covid-embeddings-simple-code.py very closely
  dataset = PaperAbstractDataset(paper_ids_with_abstracts, input_ids, attention_masks, labels)
  sampler = SequentialSampler(dataset)
  dataloader = DataLoader(dataset, sampler=sampler, batch_size=args.batch_size)

  model.cuda()
  model.eval()

  token_to_embedding_map = defaultdict(list)
  seed_embeddings = defaultdict(list)
  # number of times a token is encountered: needed to maintain the average
  token_count = defaultdict(int)
  # Measure the total training time for the whole run.
  total_t0 = time.time()
  t0 = time.time()

  logger.info("")
  logger.info('Forward pass...')

  for step, batch in enumerate(dataloader):

    if step % 100 == 0:
      logger.info('======== Batch {:} / {:} ========'.format(step, len(dataloader)))

    # batch[0] is paper_ids which are not used in finetuning and are not Tensors
    batch = tuple(t.to("cuda") for t in batch[1:])

    inputs = {
      "input_ids": batch[0],
      "attention_mask": batch[1],
    }

    logits, hidden_states = model(**inputs)

    embeddings, layers = hidden_states[0].detach().cpu(), hidden_states[1].detach().cpu()

    ##### delete unnecessary variables
    # not interested in labels sp delete them
    labels = batch[2].detach().cpu()
    paper_ids = batch[0]
    del labels
    del paper_ids
    # not interested in the logits (only embedding needed) so delete them
    del logits
    del layers
    torch.cuda.empty_cache()

    input_ids_np = inputs["input_ids"].cpu().numpy()
    embeddings_np = embeddings.detach().cpu().numpy()

    # logger.info(f"\n***** input_ids_np.shape: {input_ids_np.shape} *****")   ==> input_ids_np.shape: (4, 512)
    # logger.info(f"\n***** embeddings_np.shape: {embeddings_np.shape} *****") ==> embeddings_np.shape: (4, 512, 768)

    del inputs["input_ids"]
    del inputs["attention_mask"]
    del embeddings
    torch.cuda.empty_cache()

    for batch_number in range(len(input_ids_np)):
      tokens = tokenizer.convert_ids_to_tokens(input_ids_np[batch_number])
      for token, embedding in zip(tokens, embeddings_np[batch_number]):
        # add the seed word to the seed dict
        if token in seed_words:
          if token not in seed_embeddings:
            seed_embeddings[token] = embedding
          else:
            seed_embeddings[token] += embedding
        # every token including seed should also be added to token_to_embedding_map
        if token not in token_to_embedding_map and token not in stop_words:
          token_to_embedding_map[token] = embedding
          tokens_with_embeddings.add(token)
        elif token not in stop_words:
          token_to_embedding_map[token] += embedding
        token_count[token] += 1

    if step % 1000 == 0 and step > 0:
      with open(f'word_embeddings/{args.output_dir}/word_embeddings_averaged_{step}.pickle', 'wb') as handle:
        pickle.dump(token_to_embedding_map, handle, protocol=pickle.HIGHEST_PROTOCOL)
      del token_to_embedding_map
      token_to_embedding_map = defaultdict(list)
      logger.info("Time to find embeddings for batches {} to {}: {:} (h:mm:ss)".format(max(0, step - 500), step, format_time(time.time() - t0)))
      t0 = time.time()

    del input_ids_np
    del embeddings_np

  # save the embeddings of the seed words
  for token, embedding in seed_embeddings.items():
    seed_embeddings[token] = embedding / (token_count[token] * 1.0)
  with open(f'word_embeddings/{args.output_dir}/seed_embeddings_averaged.pickle', 'wb') as handle:
    pickle.dump(seed_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)
  del seed_embeddings

  # save the word embeddings
  with open(f'word_embeddings/{args.output_dir}/word_embeddings_averaged_{step}.pickle', 'wb') as handle:
    pickle.dump(token_to_embedding_map, handle, protocol=pickle.HIGHEST_PROTOCOL)
  del token_to_embedding_map

  # save the number of times each token occurs
  with open(f'word_embeddings/{args.output_dir}/token_count.pickle', 'wb') as handle:
    pickle.dump(token_count, handle, protocol=pickle.HIGHEST_PROTOCOL)
  del token_count

  logger.info("Total time to complete the entire process: {:} (h:mm:ss)".format(format_time(time.time() - total_t0)))

  logger.info("\n")
  logger.info("Embeddings received!")

if __name__ == "__main__":
  main()